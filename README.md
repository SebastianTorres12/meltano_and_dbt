## Docker
```sh
docker-compose -f docker/oracle.yml up
docker-compose -f docker/postgresql.yml up
```

## Installation

### pipx installation
```sh
# install pipx and ensure it is on the path
python3 -m pip install --user pipx
python3 -m pipx ensurepath
# Be sure pipx is available on your path
source ~/.bashrc
```

### Install OS dependencies
```sh
sudo dnf install python3-devel gcc
```

### Meltono installation (v4)
```sh
pipx install meltano
```

### Oracle client installation from https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html
```sh
sudo dnf install oracle-instantclient-basic-21.21.0.0.0-1.x86_64.rpm
```


## Configuration

### Create project
```sh
meltano init oracle_to_postgres
cd oracle_to_postgres
```

### Add plugins (Oracle and PostgreSQL)
```sh
meltano add tap-oracle --variant transferwise
meltano add target-postgres --variant meltanolabs
```

### Configure source
```sh
meltano config set tap-oracle host localhost
meltano config set tap-oracle port 1521
meltano config set tap-oracle user system
meltano config set tap-oracle password oracle
meltano config set tap-oracle sid XE
```

### Configure destination
```sh
meltano config set target-postgres host localhost
meltano config set target-postgres user system
meltano config set target-postgres password postgres
meltano config set target-postgres database postgres
meltano config set target-postgres default_target_schema public
```

### Create table CUSTOMER and prepare data
```sql
CREATE TABLE CUSTOMER (
  ID NUMBER GENERATED BY DEFAULT AS IDENTITY,
  NAME VARCHAR2(100),
  CONSTRAINT pk_customer PRIMARY KEY (ID)
);

INSERT INTO CUSTOMER (NAME) VALUES ('Alice Johnson');
INSERT INTO CUSTOMER (NAME) VALUES ('Bob Smith');
INSERT INTO CUSTOMER (NAME) VALUES ('Charlie Davis');
INSERT INTO CUSTOMER (NAME) VALUES ('Diana Prince');
INSERT INTO CUSTOMER (NAME) VALUES ('Edward Norton');
INSERT INTO CUSTOMER (NAME) VALUES ('Fiona Gallagher');
INSERT INTO CUSTOMER (NAME) VALUES ('George Miller');
INSERT INTO CUSTOMER (NAME) VALUES ('Hannah Abbott');
INSERT INTO CUSTOMER (NAME) VALUES ('Ian Wright');
INSERT INTO CUSTOMER (NAME) VALUES ('Julia Roberts');

COMMIT;
```

### WARNING
If Meltano has run before, you must use this command to update the catalog.
Otherwise, new tables will not be found.

```sh
meltano select tap-oracle --refresh-catalog --list --all
```


### Test source (Oracle)
```sh
meltano select tap-oracle --list --all
```

### Set CUSTOMER table and replication method
```sh
meltano select tap-oracle SYSTEM-CUSTOMER "*"
meltano config set tap-oracle default_replication_method FULL_TABLE
```

## Execution

### Run replication
```sh
meltano run tap-oracle target-postgres
```

### Verification in destination (PostgreSQL)
```sql
select * from "CUSTOMER";
```

## DBT 
### Installation

```sh
meltano add dbt-postgres
```
### Initialize DBT

```sh
meltano invoke dbt-postgres:initialize
```

This command create the dbt structure that meltano expect in the folder "transform" in the root directory

### Source tables
In the models directory it is necessary to create a file called "sources.yml" that references the tables that will be used as resources for transformations with DBT

```yml
version: 2
sources:
  - name: oracle_raw
    schema: public  # El schema donde target-postgres dejó la data
    tables:
      - name: CUSTOMER
```

### Configuration DBT

Add this lines in the part of utilities in meltano.yml

```yml
config:
      dbname: postgres
      host: localhost
      schema: analytics
      user: system    
      password: postgres
      port: 5432
```

### Creation of tables tranformations models
#### SQL FILE

In the models directory, we define the SQL files that determine the final table names in the target schema after transformation. To ensure data integrity in PostgreSQL, we have implemented dbt Model Contracts by setting contract: {enforced: true} within the model configuration. This setup mandates that the SQL output aligns perfectly with the schema defined in the .yml file, allowing dbt to physically enforce PRIMARY KEY and NOT NULL constraints in the database while preventing the execution if there are any data type mismatches.

```sql
{{ config(
    materialized='table',
    contract={'enforced': true}
) }}

SELECT
    "ID" as customer_id,
    "NAME" as name,
    CASE
        WHEN _sdc_deleted_at IS NOT NULL THEN FALSE
        ELSE TRUE
        END AS status
FROM {{ source('oracle_raw', 'CUSTOMER') }}
```
#### YML File

After creating the SQL model, a corresponding .yml file must be created—typically with the same name—to define the table's schema. This file is essential for specifying the data_type for every column and declaring the primary_key and not_null constraints. These definitions serve as the 'blueprint' that dbt uses to physically enforce data integrity in PostgreSQL when the model contract is enabled.

```yml
version: 2

models:
  - name: customer_final
    description: "Tabla con PK real en Postgres"
    columns:
      - name: customer_id
        data_type: bigint
        constraints:
          - type: primary_key
          - type: not_null

      - name: name
        data_type: varchar

      - name: status
        data_type: boolean
```

### Test DBT

```sh
meltano invoke dbt-postgres debug
```

### Run DBT
```sh
meltano invoke dbt-postgres:run
```

### Run Singer and DBT

```sh
meltano run tap-oracle target-postgres dbt-postgres:run
```



## Incremental DBT (Change Data Detection)

To do this, we need to add a new row in the final table named "hash", this value let the compare the entrance data and the original table data,
this value is created by a combination of the key fields in the table, this let dbt compare this values to update the data.

To do this, we need to define the contract with this row.

```yml
version: 2

models:
  - name: customer
    description: "Tabla final de clientes con contratos y tipos de datos validados"
    config:
      contract:
        enforced: true  # Esto obliga a dbt a validar los tipos de datos en cada ejecución
    columns:
      - name: address_number
        description: "ID principal del cliente (PK)"
        data_type: integer
        constraints:
          - type: primary_key
          - type: not_null

      - name: name
        description: "Nombre legal o comercial"
        data_type: varchar(255)
        constraints:
          - type: not_null

      - name: ruc
        description: "Registro Único de Contribuyentes (Ecuador)"
        data_type: varchar(13)
        constraints:
          - type: not_null

      - name: credit_limit
        description: "Cupo máximo de crédito asignado"
        data_type: decimal(12,2)

      - name: payment_method
        description: "Forma de pago o días de crédito"
        data_type: varchar(50)

      - name: active
        description: "Estado lógico del cliente"
        data_type: boolean
        constraints:
          - type: not_null

      - name: created_at
        description: "Fecha de creación del registro"
        data_type: timestamp
        constraints:
          - type: not_null

      - name: updated_at
        description: "Fecha de última modificación"
        data_type: timestamp
        constraints:
          - type: not_null
      - name: row_hash
        description: "Hash de validación para detectar cambios"
        data_type: text # O varchar(32)
```

And in the sql file, this conditionals are needed, the hash value is formed here and the comparition too.

```sql
{{ config(
    materialized='incremental',
    unique_key='address_number',
    on_schema_change='fail',
    contract={'enforced': true}
) }}

WITH raw_data AS (
    SELECT
        CAST("COD_JDE" AS INTEGER) AS address_number,
        CAST(TRIM("NOMBRE") AS VARCHAR(255)) AS name,
        CAST(TRIM("RUC") AS VARCHAR(13)) AS ruc,
        CAST("CUPO" AS DECIMAL(12,2)) AS credit_limit,
        CAST(TRIM("FORMAPAGO") AS VARCHAR(50)) AS payment_method,
        -- Variable intermedia para el estado
        CASE
            WHEN _sdc_deleted_at IS NULL THEN TRUE
            ELSE FALSE
        END AS is_active,
        CAST(_sdc_extracted_at AS TIMESTAMP) AS extraction_date
    FROM {{ source('oracle_raw_clientes', 'CLIENTES_B2B') }}
),

processed_data AS (
    SELECT
        *,
        -- INCLUIMOS 'is_active' en el HASH
        MD5(CONCAT(
            name,
            ruc,
            CAST(credit_limit AS TEXT),
            payment_method,
            CAST(is_active AS TEXT) -- Si esto cambia de TRUE a FALSE, el MD5 cambia
        )) AS row_hash
    FROM raw_data
)

SELECT
    pd.address_number,
    pd.name,
    pd.ruc,
    pd.credit_limit,
    pd.payment_method,
    CAST(pd.is_active AS BOOLEAN) AS active,
    CAST(
        {% if is_incremental() %}
            COALESCE(dest.created_at, pd.extraction_date)
        {% else %}
            pd.extraction_date
        {% endif %}
        AS TIMESTAMP
    ) AS created_at,
    CAST(
        {% if is_incremental() %}
            CASE
                WHEN pd.row_hash != dest.row_hash THEN CURRENT_TIMESTAMP
                ELSE dest.updated_at
            END
        {% else %}
            CURRENT_TIMESTAMP
        {% endif %}
        AS TIMESTAMP
    ) AS updated_at,
    CAST(pd.row_hash AS TEXT) AS row_hash
FROM processed_data pd
    {% if is_incremental() %}
    LEFT JOIN {{ this }} dest ON pd.address_number = dest.address_number
WHERE dest.address_number IS NULL OR pd.row_hash != dest.row_hash
    {% endif %}
```

materialized='incremental': This tells dbt not to drop and recreate the table from scratch every time. 
Instead, it only processes and adds "new" or "updated" data since the last run, significantly improving performance for large datasets.

unique_key='address_number': This serves as the "anchor" to prevent duplicates. 
It tells dbt which column to use to identify existing records; if a record with the same ID arrives, dbt will update it instead of creating a second copy.

on_schema_change='fail': This is a structural safety net. 
If dbt detects that the table structure in the source has changed (e.g., a column was removed or a data type changed), it will immediately stop the process to prevent corrupting your final table.

contract={'enforced': true}: This enables strict data validation. 
It forces dbt to ensure that your SQL query results match the exact columns and data types defined in your YAML file before allowing any data to be written.